[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Yumeka Nagano‚Äôs Blog",
    "section": "",
    "text": "Welcome üëã\nThis is my personal blog about reinforcement learning and control.\nHere are some posts you can read:\n\nUnderstanding Soft Actor-Critic: Entropy-Constrained Reinforcement Learning\n\n\nI‚Äôll be adding more posts soon!"
  },
  {
    "objectID": "sac.html",
    "href": "sac.html",
    "title": "Understanding Soft Actor-Critic: Entropy-Constrained Reinforcement Learning",
    "section": "",
    "text": "In this post, I explore the key ideas behind the Soft Actor-Critic algorithm ‚Äî how it uses entropy regularization to achieve stable and efficient learning, and why its automatic temperature adjustment is a breakthrough in continuous control RL.\n\n\nIntroduction\nThe Soft Actor-Critic (SAC) algorithm has become one of the most reliable and widely used methods in deep reinforcement learning (RL), especially for continuous control tasks.\nAt first glance, SAC may look similar to other actor-critic algorithms like DDPG or TD3‚Äîbut its key innovation lies in how it optimizes not just reward, but also entropy.\nIn this post, we‚Äôll explore the main ideas behind SAC as introduced in Soft Actor-Critic Algorithms and Applications (Haarnoja et al., 2018).\nWe‚Äôll focus on two core features that make SAC both stable and efficient:\n\nEntropy-constrained learning: SAC doesn‚Äôt just balance reward and entropy with a fixed coefficient‚Äîit enforces a target entropy constraint.\n\nAutomatic temperature adjustment: The temperature parameter Œ± is learned, not manually tuned.\n\nBy the end, we‚Äôll see why this approach makes SAC more robust, sample-efficient, and less sensitive to hyperparameters than many prior methods.\n\n\n\nMotivation: Beyond Deterministic Control\nTraditional actor-critic algorithms (like DDPG) are deterministic: the policy outputs a specific action for each state.\nThis is efficient for exploitation, but risky for exploration‚Äîespecially in continuous action spaces.\nEntropy-regularized reinforcement learning introduces a stochastic policy that maximizes not only expected reward but also policy entropy:\n\\[\nJ(\\pi) = \\mathbb{E}_{(s,a)\\sim\\rho_\\pi} [r(s,a) + \\alpha \\mathcal{H}(\\pi(\\cdot|s))]\n\\]\nHere:\n\n( ((|s)) = - _{a} [(a|s)] ) measures how random the policy is.\n( ) controls the trade-off between reward and entropy.\n\nHigher entropy encourages exploration and smoother learning, but if ( ) is too high, the agent behaves almost randomly.\nSo, how do we pick Œ±?\n\n\n\nKey Structure\n\n\n\n\nFixed vs.¬†Adaptive Temperature\nEarly versions of entropy-regularized RL used a fixed Œ±, chosen via hyperparameter tuning.\nThis is suboptimal for two reasons:\n\nThe ‚Äúright‚Äù level of entropy depends on the environment and the stage of learning.\n\nA fixed Œ± couples exploration strength directly to reward scaling.\n\nSAC resolves this beautifully by treating Œ± as a learnable parameter.\n\n\n\nAutomatic Temperature Adjustment\nThe SAC paper reframes Œ± as a variable that enforces a target entropy constraint:\n\\[\n\\mathcal{L}(\\alpha) = \\mathbb{E}_{a_t \\sim \\pi_t} [-\\alpha (\\log \\pi_t(a_t|s_t) + \\mathcal{H}_{\\text{target}})]\n\\]\nThis objective pushes Œ± to increase if the policy entropy is below the target and decrease otherwise.\nIn PyTorch, the update step looks like this:\n\n\nCode\nimport torch\n\n# Initialize log_alpha (so Œ± = exp(log_alpha))\nlog_alpha = torch.tensor(0.0, requires_grad=True)\nalpha_optimizer = torch.optim.Adam([log_alpha], lr=3e-4)\n\n# Example of computing the temperature loss\nlog_prob = torch.tensor([-1.2, -0.8, -1.5])  # log œÄ(a|s) for some batch\ntarget_entropy = -1.0  # typical target for 1D continuous action\n\nalpha_loss = -(log_alpha * (log_prob + target_entropy).detach()).mean()\nalpha_optimizer.zero_grad()\nalpha_loss.backward()\nalpha_optimizer.step()\n\nalpha = log_alpha.exp().item()\nalpha\n\n\n0.9997000694274902\n\n\nThe optimizer adjusts log_alpha so that the expected policy entropy matches the desired target.\n\n\n\nWhy Entropy Constraint Matters\nInstead of weighting reward and entropy with a fixed scalar Œ±, SAC enforces:\n\\[\n\\mathcal{H}(\\pi(\\cdot|s_t)) \\approx \\mathcal{H}_{\\text{target}}\n\\]\nThis dynamic adjustment stabilizes learning in several ways:\n\nPrevents premature convergence (too little exploration).\nAvoids oscillation or instability from excessive randomness.\nKeeps policy behavior consistent across environments and reward scales.\n\nIntuitively, it‚Äôs like giving the agent an adaptive curiosity level that balances exploration with exploitation automatically.\n\n\n\nEntropy‚ÄìTemperature Relationship\nThe relationship between entropy and Œ± can be visualized simply:\n\n\nCode\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nentropies = np.linspace(0.5, 2.0, 100)\nalphas = 1 / entropies\n\nplt.plot(entropies, alphas)\nplt.xlabel(\"Entropy\")\nplt.ylabel(\"Temperature Œ±\")\nplt.title(\"Entropy‚ÄìTemperature Relationship in SAC\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nWhen the policy‚Äôs entropy decreases (less randomness), Œ± increases‚Äîpushing the policy to become more stochastic again. This feedback loop maintains a balanced level of exploration throughout training.\n\n\n\nThe Soft Q-Function\nSAC also modifies the Bellman backup to incorporate entropy:\n\\[\nQ^{\\pi}(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}{s{t+1}\\sim p, a_{t+1}\\sim\\pi} [Q^{\\pi}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi(a_{t+1}|s_{t+1})]\n\\]\nThis soft Q-function estimates the expected return plus an entropy bonus. The inclusion of ( -(a|s) ) encourages higher entropy where possible, while still optimizing reward.\n\n\n\nPolicy Update\nThe policy update is also softened:\n\\[\n\\pi_{\\text{new}} = \\arg\\min_\\pi D_{\\text{KL}}\\left(\n\\pi(\\cdot|s_t) ;||;\n\\frac{\\exp(\\frac{1}{\\alpha} Q(s_t, \\cdot))}{Z(s_t)}\n\\right)\n\\]\nThis means the optimal policy should be close to a Boltzmann distribution over actions:\n\\[\n\\pi^(a|s) \\propto \\exp\\left(\\frac{1}{\\alpha} Q^(s,a)\\right)\n\\]\nSo Œ± acts as a temperature: ‚Ä¢ High Œ± ‚Üí flatter distribution ‚Üí more exploration ‚Ä¢ Low Œ± ‚Üí sharper distribution ‚Üí more exploitation\n\n\n\nFull SAC Algorithm\nHere‚Äôs a simplified pseudo-code version of SAC:\nAlgorithm 1: Soft Actor-Critic (SAC)\nInitialize policy parameters Œ∏, Q-function parameters œÜ‚ÇÅ, œÜ‚ÇÇ, temperature Œ±\nrepeat\n    Observe state s\n    Sample action a ~ œÄ_Œ∏(a|s)\n    Execute action a, observe reward r and next state s'\n    Store (s, a, r, s') in replay buffer D\n    Sample batch B = {(s, a, r, s')} from D\n    Update critics œÜ‚ÇÅ, œÜ‚ÇÇ using Bellman backup:\n        y = r + Œ≥ * min_i Q_{œÜ_i}(s', a') - Œ± * log œÄ_Œ∏(a'|s')\n    Update actor Œ∏ using:\n        ‚àá_Œ∏ J(Œ∏) = ‚àá_Œ∏ [Œ± * log œÄ_Œ∏(a|s) - Q_œÜ(s,a)]\n    Adjust temperature Œ±:\n        ‚àá_Œ± J(Œ±) = -Œ± * (log œÄ_Œ∏(a|s) + H_target)\nuntil convergence\n\n\n\nImplementation Insights\nThe key insight in SAC is the interplay between three components: ‚Ä¢ The soft Q-function estimates value under entropy regularization. ‚Ä¢ The policy network seeks to minimize the expected Œ±-weighted KL divergence. ‚Ä¢ The temperature Œ± adapts automatically to maintain a fixed entropy target.\nTogether, these make SAC: ‚Ä¢ Stable: fewer learning crashes compared to DDPG/TD3. ‚Ä¢ Efficient: reuses experience through an off-policy replay buffer. ‚Ä¢ Automatic: minimal hyperparameter tuning for entropy.\n\n\n\nPractical Considerations\n‚Ä¢   Target Entropy:\nOften set to ( _{} = -(A) ), where A is the action space dimension. For a 1D action space, that‚Äôs ‚àí1.0; for 6D, ‚àí6.0. ‚Ä¢ Twin Q-Networks: SAC uses two critics (like TD3) to mitigate overestimation bias. ‚Ä¢ Reparameterization Trick: Actions are sampled as ( a = (s) + (s), ; (0,1) ), allowing gradients to flow through stochastic actions.\n\n\n\nVisualization Example: Entropy Change Over Training\nTo get a sense of how Œ± evolves, here‚Äôs a toy example simulating Œ± adapting as entropy changes:\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsteps = np.arange(200)\ntarget_entropy = -1.0\nlog_alpha = 0.0\nalpha = np.exp(log_alpha)\n\nentropies = np.linspace(0.0, -2.0, len(steps))\nalphas = []\n\nfor entropy in entropies:\n    alpha_loss = -(log_alpha * (entropy - target_entropy))\n    log_alpha -= 0.01 * alpha_loss  # gradient step\n    alpha = np.exp(log_alpha)\n    alphas.append(alpha)\n\nplt.plot(steps, alphas)\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Alpha Value\")\nplt.title(\"Adaptive Temperature Œ± Over Time\")\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\nYou‚Äôll see Œ± increase when entropy drops below the target and decrease when it‚Äôs too high ‚Äî a perfect self-correcting mechanism.\n\n\n\nWhy It Works So Well\nSAC‚Äôs success stems from combining three major strengths: 1. Entropy regularization improves exploration. 2. Off-policy updates improve sample efficiency. 3. Automatic temperature tuning stabilizes learning.\nThis triad makes SAC remarkably robust and effective in diverse continuous control benchmarks like HalfCheetah, Hopper, and Walker2d.\n\n\n\nConclusion\nSoft Actor-Critic represents a fundamental shift in reinforcement learning design. By optimizing for expected reward under a constraint on entropy, SAC provides a principled, practical way to achieve stable and efficient learning.\nKey takeaways: ‚Ä¢ SAC maximizes both reward and uncertainty. ‚Ä¢ The temperature Œ± is learned, not tuned. ‚Ä¢ Entropy constraints make learning more adaptive and stable.\nIf you‚Äôve struggled with brittle RL agents that either overfit or fail to explore, SAC‚Äôs entropy-constrained formulation offers a robust, elegant solution.\n\n\n\nReferences\n‚Ä¢   Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018).\nSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor. [Paper] ‚Ä¢ Haarnoja, T., et al.¬†(2019). Soft Actor-Critic Algorithms and Applications. [Paper]"
  }
]