[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to Yumeka Nagano’s Blog",
    "section": "",
    "text": "Welcome 👋\nThis is my personal blog about reinforcement learning and control.\nHere are some posts you can read:\n\nUnderstanding Soft Actor-Critic: Entropy-Constrained Reinforcement Learning\n\n\nI’ll be adding more posts soon!"
  },
  {
    "objectID": "sac.html",
    "href": "sac.html",
    "title": "Understanding Soft Actor-Critic: Entropy-Constrained Reinforcement Learning",
    "section": "",
    "text": "In this post, I explore the key ideas behind the Soft Actor-Critic algorithm—how it leverages entropy regularization to achieve stable and efficient learning, and why its automatic temperature adjustment represents a breakthrough in continuous control reinforcement learning.\n\n\nIntroduction\nThe Soft Actor-Critic (SAC) algorithm has become one of the most widely used methods in deep reinforcement learning (RL), especially for continuous control tasks.\nAt first glance, SAC may look similar to other actor-critic algorithms like DDPG (Lillicrap et al. 2015) or TD3 (Fujimoto, Hoof, and Meger 2018)—but its key innovation lies in how it optimizes both reward and entropy.\nThis post explores the main ideas behind SAC as presented in Soft Actor-Critic Algorithms and Applications (Haarnoja et al. 2019), which builds upon the earlier work Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor (Haarnoja et al. 2018).\nI focus on two core features that make SAC both stable and efficient:\n\nEntropy-regulated learning: SAC doesn’t just balance reward and entropy with a fixed coefficient—it enforces a target entropy constraint.\n\nAutomatic temperature adjustment: The temperature parameter \\(\\alpha\\) is learned rather than manually tuned.\n\nBy the end of this post, we’ll see how these design choices make SAC more robust, sample-efficient, and less sensitive to hyperparameters than many of its predecessors.\n\n\n\nBackground\nRL has evolved through a series of key algorithmic advances in both on-policy and off-policy methods. Understanding these developments provides the foundation for why the SAC algorithm was introduced.\n\nOn-Policy Methods\nEarly on-policy algorithms such as TRPO (Schulman et al. 2015), PPO (Schulman et al. 2017), and A3C (Mnih et al. 2016) directly optimize policies using trajectories collected from the most recent policy. While effective, these approaches require fresh samples for every gradient step, which makes them sample-inefficient, particularly in continuous control tasks where data collection is expensive.\n\n\nDeterministic Off-Policy Methods\nDDPG (Lillicrap et al. 2015) improved sample efficiency by introducing deterministic actors and leveraging experience replay. However, DDPG is sensitive to hyperparameters and often unstable in high-dimensional or complex environments.\nTD3 (Fujimoto, Hoof, and Meger 2018) addressed several of DDPG’s weaknesses by introducing three key modifications:\n\nTwin critics to reduce overestimation bias\nDelayed actor updates for smoother training\nTarget policy smoothing to improve stability\n\nThese changes made TD3 more robust and reliable, yet it still relied on a deterministic actor, limiting its ability to explore effectively in challenging tasks.\n\n\nMaximum Entropy and Stochastic Exploration\nThe maximum entropy framework (Ziebart et al. 2008) proposed augmenting the RL objective with an entropy term, encouraging agents to favor stochastic policies that promote exploration and robustness.\nBuilding on this idea, Soft Q-Learning (Haarnoja et al. 2017) applied entropy maximization to continuous control tasks. However, it often struggled with stability because it did not explicitly integrate actor–critic training.\nSimilarly, algorithms like PPO include an entropy bonus to encourage exploration, but the entropy weight is typically a fixed, manually tuned parameter, which limits adaptability.\n\n\nMilestones in Continuous Control RL\nHere’s a simplified timeline showing key advances leading up to SAC:\n\n\nDQN | DDPG | TD3 | SAC | CQL/Dreamer\n\n\nDQN: First deep RL with value-based learning for discrete actions.\n\nDDPG: Off-policy deterministic actor–critic for continuous control.\n\nTD3: Improved DDPG with twin critics and smoothing.\n\nThese developments set the stage for SAC, which unifies off-policy learning with stochastic exploration and adaptive entropy.\n\n\n\n\n\n\nTipWhy SAC?\n\n\n\nSAC unifies these ideas around entropy into a single, practical framework by:\n\nEmploying a stochastic actor to enhance exploration\nIntroducing a learnable temperature parameter to automatically balance reward and entropy\nExplicitly modifying the value function to incorporate the entropy term\n\nThese innovations make SAC both stable and sample-efficient, even in high-dimensional continuous control tasks, addressing limitations of previous methods.\n\n\n\n\n\n\nHow SAC Works\nEntropy-regularized reinforcement learning introduces a stochastic policy that maximizes not only expected reward but also policy entropy:\n\\[\nJ(\\pi) = \\sum_{t=0}^{T}\\mathbb{E}_{(s_t,a_t)\\sim\\rho_\\pi} [r(s_t,a_t) + \\alpha \\mathcal{H}(\\pi(\\cdot|s_t))]\n\\]\nHere:\n\n\\(\\mathcal{H}\\) measures how random the policy is.\n\\(\\alpha\\) controls the trade-off between reward and entropy.\n\nWith this objective, SAC consists of the following key components:\n\nActor network: The decision-maker that outputs a distribution over actions, parameterized by its mean and standard deviation. This makes SAC a stochastic actor, unlike deterministic policies such as DDPG or TD3.\nCritic networks: Two critic networks independently estimate the action-value function. The minimum of their Q-values is used to update the policy, reducing overestimation bias (following the Double Q-learning idea (Hasselt 2010)).\nReplay buffer: Enables sample-efficient learning by reusing past transitions rather than collecting fresh samples every update.\nEntropy regularization: Encourages exploration by using the actor’s output to compute policy entropy \\(\\mathcal{H}\\), which directly influences the critic updates.\n\n\n\n\n\nAutomatic Temperature Adjustment\nEarlier entropy-regularized RL algorithms, including the original SAC paper (Haarnoja et al. 2018), used a fixed temperature parameter (\\(\\alpha\\)) that had to be manually tuned to balance exploration and exploitation. However, this approach has two major drawbacks:\n\nThe optimal entropy level depends on the task and stage of learning.\n\nA fixed \\(\\alpha\\) ties exploration strength to reward scaling, making tuning difficult and inconsistent.\n\nSAC resolves this issue by treating \\(\\alpha\\) as a learnable parameter. It reframes the problem as enforcing a target entropy constraint, formulated as a constrained optimization problem:\n\\[\n\\max_{\\pi_{0:T}} \\mathbb{E}_{\\rho_\\pi} \\left[ \\sum_{t=0}^{T} r(s_t, a_t) \\right]\n\\quad \\text{s.t.} \\quad\n\\mathbb{E}_{(s_t, a_t) \\sim \\rho_\\pi} \\big[ -\\log(\\pi_t(a_t \\mid s_t)) \\big] \\ge \\mathcal{H}, \\ \\forall t\n\\]\nThis constrained maximization is converted to a dual problem, where the optimal dual variable \\(\\alpha_t^*\\) satisfies:\n\\[\n\\alpha_t^* = \\arg\\min_{\\alpha_t} \\mathbb{E}_{a_t \\sim \\pi_t^*}\n\\left[ -\\alpha_t \\log \\pi_t^*(a_t \\mid s_t; \\alpha_t) - \\alpha_t \\bar{\\mathcal{H}} \\right]\n\\]\nIn practice, \\(\\alpha\\) is learned by minimizing the following objective:\n\\[\nJ(\\alpha) = \\mathbb{E}_{a_t \\sim \\pi_t}\n\\left[ -\\alpha \\log \\pi_t(a_t \\mid s_t) - \\alpha \\bar{\\mathcal{H}} \\right]\n\\]\nThis adaptive temperature learning, together with policy and soft Q-function updates, forms the core mechanism behind SAC’s stability and efficiency.\nHere’s a pseudo code of SAC:\n\nImage credit: Soft Actor-Critic Algorithms and Applications (Haarnoja et al. 2019)\n\n\n\nResults\nThe algorithm was evaluated in both simulation and real-world robotic settings.\n\nMuJoCo Simulation\nThis environment serves as a standard benchmark for continuous control tasks.\nThe following algorithms were used for comparison:\n\nDDPG – One of the early efficient off-policy deep RL methods.\n\nTD3 – An extension of DDPG that applies the Double Q-learning idea to continuous control.\n\nPPO – A stable and widely used on-policy policy gradient algorithm.\n\nThe authors compared versions of SAC using both a fixed (manually tuned) and an automatically learned temperature parameter, \\(\\alpha\\).\nHere are the learning curves across different environments. You can also find the corresponding videos at the end of this article.\n Training curves on benchmark tasks. Image credit: Soft Actor-Critic Algorithms and Applications (Haarnoja et al. 2019).\nThese results show that SAC consistently performs as well as—or better than—the baselines.\nIt learns faster in terms of environment interaction steps and exhibits more stable convergence in most cases.\n\n\nReal-world Robotics\nThe algorithm was also evaluated on challenging real-world robotic tasks: quadrupedal locomotion and dexterous hand manipulation.\n\n\n\n Locomotion (image credit: Soft Actor-Critic).\n\n\n Manipulation (image credit: Soft Actor-Critic)\n\n\n\nSAC successfully learned both tasks.\nA particularly remarkable finding is that the locomotion policy was trained only on flat terrain, yet it generalized to walking on slopes and steps.\nSimilarly, the manipulation policy learned faster than PPO and demonstrated strong sample efficiency.\n\n\nWhy It Works So Well?\nSAC’s success combines:\n\nEntropy regularization → better exploration\n\nOff-policy updates → sample efficiency\n\nAutomatic temperature tuning → stable learning\n\n\n\n\n\nImpact and Legacy\nThe paper has been cited 4,018 times (as of October 23, 2025), and the original paper has 12,834 citations according to Google Scholar, underscoring SAC’s profound influence on reinforcement learning research.\nSAC represents a turning point in modern RL. By combining off-policy actor–critic methods with entropy-regularized objectives, it set a new standard for stable, efficient, and generalizable learning in continuous control tasks.\n\nInfluence on Future Work\nSAC inspired a wave of follow-up research:\n\nCQL (Conservative Q-Learning): Builds on SAC’s stochastic actor–critic structure for offline RL.\n\nDreamer: Incorporates model-based planning with entropy-regularized objectives.\n\nOther entropy-regularized methods: Many modern continuous control algorithms adopt SAC-style stochastic actors and adaptive entropy tuning as standard practice.\n\nBy unifying practical learning methods with principled exploration, SAC continues to serve as a benchmark for evaluating new RL algorithms in robotics, control, and reinforcement learning research.\n\n\n\n\nSAC vs. PPO\nHere’s my recommendation for when to use SAC versus PPO.\n\n\n\nWhen to use SAC?\n\nData is limited.\n\nTask involves continuous actions.\n\n\n\nWhen to use PPO?\n\nTask has discrete or low-dimensional actions.\nYou prefer an easy-to-implement on-policy method.\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIf you’re unsure which to choose — try both and compare!\n\n\n\n\n\nCritiques\nHere are my personal reflections on the paper.\n\n\n\n\n\n\nTipStrengths\n\n\n\n\nElegant formulation combining entropy maximization with actor–critic learning.\n\nConcise and well-structured writing.\n\nReal-world experiments — most RL papers are limited to simulation, so building physical setups adds great credibility.\n\nHighly influential and extensible — inspired many later algorithms like CQL and Dreamer.\n\n\n\n\n\n\n\n\n\nWarningLimitations\n\n\n\n\nLacks intuitive figures or conceptual illustrations, making the algorithm harder to fully grasp.\n\nOmits computational cost details for MuJoCo simulations — other methods might be faster in wall-clock training time.\n\nLittle discussion on humanoid task stability — especially regarding the automatically adjusted temperature version.\n\nLocomotion tasks involve hand-tuned penalties, which may reduce generality.\n\nLimited experimental diversity — most tests are MuJoCo simulations or a few specific real-world setups; the valve manipulation task seems unusual.\n\nFew ablations — effects of network architecture, replay buffer size, and entropy targets are not systematically analyzed.\n\n\n\n\n\n\nConclusion\nThe Soft Actor-Critic algorithm represents a fundamental shift in reinforcement learning design.\nBy optimizing expected reward under an entropy constraint, SAC offers a principled and practical solution for achieving both stability and efficiency.\nKey takeaways:\n\nSAC maximizes both reward and uncertainty.\n\nThe temperature parameter \\(\\alpha\\) is learned, not manually tuned.\n\nEntropy constraints make learning more adaptive and stable.\n\nIf you’ve ever struggled with brittle RL agents that overfit or fail to explore, SAC’s entropy-constrained framework provides a robust, elegant solution.\n\n\n\nVideos\nThese videos demonstrate SAC’s performance in MuJoCo simulations (from the original paper) and in real-world robotic environments (image/video credit: Soft Actor-Critic).\n\nHopperWalker 2DHalf CheetahAntHumanoidHumanoid (rllab)\n\n\n\n\nYour browser does not support the video tag. \n\n\n\n\nYour browser does not support the video tag. \n\n\n\n\nYour browser does not support the video tag. \n\n\n\n\nYour browser does not support the video tag. \n\n\n\n\nYour browser does not support the video tag. \n\n\n\n\nYour browser does not support the video tag. \n\n\n\n\n\n\n\n\n\nReferences\n\nFujimoto, Scott, Herke van Hoof, and David Meger. 2018. “Addressing Function Approximation Error in Actor-Critic Methods.” In Proceedings of the 35th International Conference on Machine Learning, edited by Jennifer Dy and Andreas Krause, 80:1587–96. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v80/fujimoto18a.html.\n\n\nHaarnoja, Tuomas, Haoran Tang, Pieter Abbeel, and Sergey Levine. 2017. “Reinforcement Learning with Deep Energy-Based Policies.” In Proceedings of the 34th International Conference on Machine Learning, edited by Doina Precup and Yee Whye Teh, 70:1352–61. Proceedings of Machine Learning Research. PMLR. https://proceedings.mlr.press/v70/haarnoja17a.html.\n\n\nHaarnoja, Tuomas, Aurick Zhou, Pieter Abbeel, and Sergey Levine. 2018. “Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.” August 8, 2018. https://doi.org/10.48550/arXiv.1801.01290.\n\n\nHaarnoja, Tuomas, Aurick Zhou, Kristian Hartikainen, George Tucker, Sehoon Ha, Jie Tan, Vikash Kumar, et al. 2019. “Soft Actor-Critic Algorithms and Applications.” January 29, 2019. https://doi.org/10.48550/arXiv.1812.05905.\n\n\nHasselt, Hado. 2010. “Double q-Learning.” In Advances in Neural Information Processing Systems, edited by J. Lafferty, C. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta. Vol. 23. Curran Associates, Inc. https://proceedings.neurips.cc/paper_files/paper/2010/file/091d584fced301b442654dd8c23b3fc9-Paper.pdf.\n\n\nLillicrap, Timothy P., Jonathan J. Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. 2015. “Continuous Control with Deep Reinforcement Learning.” 2015. https://doi.org/10.48550/ARXIV.1509.02971.\n\n\nMnih, Volodymyr, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. 2016. “Asynchronous Methods for Deep Reinforcement Learning.” In Proceedings of the 33rd International Conference on Machine Learning, edited by Maria Florina Balcan and Kilian Q. Weinberger, 48:1928–37. Proceedings of Machine Learning Research. New York, New York, USA: PMLR. https://proceedings.mlr.press/v48/mniha16.html.\n\n\nSchulman, John, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. 2015. “Trust Region Policy Optimization.” In Proceedings of the 32nd International Conference on Machine Learning, edited by Francis Bach and David Blei, 37:1889–97. Proceedings of Machine Learning Research. Lille, France: PMLR. https://proceedings.mlr.press/v37/schulman15.html.\n\n\nSchulman, John, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. “Proximal Policy Optimization Algorithms.” 2017. https://doi.org/10.48550/ARXIV.1707.06347.\n\n\nZiebart, Brian D, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. 2008. “Maximum Entropy Inverse Reinforcement Learning.” In Aaai, 8:1433–38. Chicago, IL, USA."
  }
]