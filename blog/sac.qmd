---
title: "Understanding Soft Actor-Critic: Entropy-Constrained Reinforcement Learning"
author: "Yumeka Nagano"
format:
  html:
    toc: true
    # toc-location: left
    toc-depth: 3
    code-fold: true
    theme: flatly
jupyter: python3
execute:
  echo: true
  eval: true
---

In this post, I explore the key ideas behind the Soft Actor-Critic algorithm — how it uses entropy regularization to achieve stable and efficient learning, and why its automatic temperature adjustment is a breakthrough in continuous control RL.

![](images/intro2.png){width=100%}

# Introduction

The **Soft Actor-Critic (SAC)** algorithm has become one of the most reliable and widely used methods in deep reinforcement learning (RL), especially for continuous control tasks.  
At first glance, SAC may look similar to other actor-critic algorithms like DDPG or TD3—but its key innovation lies in how it **optimizes not just reward, but also entropy**.

In this post, we’ll explore the main ideas behind SAC as introduced in *Soft Actor-Critic Algorithms and Applications* (Haarnoja et al., 2018).  
We’ll focus on two core features that make SAC both stable and efficient:

1. **Entropy-constrained learning:** SAC doesn’t just balance reward and entropy with a fixed coefficient—it enforces a target entropy constraint.  
2. **Automatic temperature adjustment:** The temperature parameter α is learned, not manually tuned.

By the end, we’ll see why this approach makes SAC more robust, sample-efficient, and less sensitive to hyperparameters than many prior methods.

---

# Motivation: Beyond Deterministic Control

Traditional actor-critic algorithms (like DDPG) are **deterministic**: the policy outputs a specific action for each state.  
This is efficient for exploitation, but risky for exploration—especially in continuous action spaces.

Entropy-regularized reinforcement learning introduces a **stochastic policy** that maximizes not only expected reward but also **policy entropy**:

$$
J(\pi) = \mathbb{E}_{(s,a)\sim\rho_\pi} [r(s,a) + \alpha \mathcal{H}(\pi(\cdot|s))]
$$

Here:

- \( \mathcal{H}(\pi(\cdot|s)) = - \mathbb{E}_{a\sim\pi} [\log \pi(a|s)] \) measures how random the policy is.
- \( \alpha \) controls the trade-off between reward and entropy.

Higher entropy encourages exploration and smoother learning, but if \( \alpha \) is too high, the agent behaves almost randomly.  
So, how do we pick α?

---

# Key Structure
![](images/structure.png){width=100%}

---

# Fixed vs. Adaptive Temperature

Early versions of entropy-regularized RL used a **fixed α**, chosen via hyperparameter tuning.  
This is suboptimal for two reasons:

1. The “right” level of entropy depends on the environment and the stage of learning.  
2. A fixed α couples exploration strength directly to reward scaling.

SAC resolves this beautifully by treating α as a **learnable parameter**.

---

# Automatic Temperature Adjustment

The SAC paper reframes α as a variable that enforces a **target entropy constraint**:

$$
\mathcal{L}(\alpha) = \mathbb{E}_{a_t \sim \pi_t} [-\alpha (\log \pi_t(a_t|s_t) + \mathcal{H}_{\text{target}})]
$$

This objective pushes α to increase if the policy entropy is below the target and decrease otherwise.

In PyTorch, the update step looks like this:

```{python}
import torch

# Initialize log_alpha (so α = exp(log_alpha))
log_alpha = torch.tensor(0.0, requires_grad=True)
alpha_optimizer = torch.optim.Adam([log_alpha], lr=3e-4)

# Example of computing the temperature loss
log_prob = torch.tensor([-1.2, -0.8, -1.5])  # log π(a|s) for some batch
target_entropy = -1.0  # typical target for 1D continuous action

alpha_loss = -(log_alpha * (log_prob + target_entropy).detach()).mean()
alpha_optimizer.zero_grad()
alpha_loss.backward()
alpha_optimizer.step()

alpha = log_alpha.exp().item()
alpha
```

The optimizer adjusts log_alpha so that the expected policy entropy matches the desired target.

---

# Why Entropy Constraint Matters

Instead of weighting reward and entropy with a fixed scalar α, SAC enforces:

$$
\mathcal{H}(\pi(\cdot|s_t)) \approx \mathcal{H}_{\text{target}}
$$

This dynamic adjustment stabilizes learning in several ways:

- Prevents premature convergence (too little exploration).
- Avoids oscillation or instability from excessive randomness.
- Keeps policy behavior consistent across environments and reward scales.

Intuitively, it’s like giving the agent an adaptive curiosity level that balances exploration with exploitation automatically.

---

# Entropy–Temperature Relationship

The relationship between entropy and α can be visualized simply:

```{python}
import matplotlib.pyplot as plt
import numpy as np

entropies = np.linspace(0.5, 2.0, 100)
alphas = 1 / entropies

plt.plot(entropies, alphas)
plt.xlabel("Entropy")
plt.ylabel("Temperature α")
plt.title("Entropy–Temperature Relationship in SAC")
plt.grid(True)
plt.show()
```

When the policy’s entropy decreases (less randomness), α increases—pushing the policy to become more stochastic again.
This feedback loop maintains a balanced level of exploration throughout training.

---

# The Soft Q-Function

SAC also modifies the Bellman backup to incorporate entropy:

$$
Q^{\pi}(s_t, a_t) = r(s_t, a_t) + \gamma \mathbb{E}{s{t+1}\sim p, a_{t+1}\sim\pi} [Q^{\pi}(s_{t+1}, a_{t+1}) - \alpha \log \pi(a_{t+1}|s_{t+1})]
$$

This soft Q-function estimates the expected return plus an entropy bonus.
The inclusion of ( -\alpha \log \pi(a|s) ) encourages higher entropy where possible, while still optimizing reward.

---

# Policy Update

The policy update is also softened:

$$
\pi_{\text{new}} = \arg\min_\pi D_{\text{KL}}\left(
\pi(\cdot|s_t) ;||;
\frac{\exp(\frac{1}{\alpha} Q(s_t, \cdot))}{Z(s_t)}
\right)
$$

This means the optimal policy should be close to a Boltzmann distribution over actions:

$$
\pi^(a|s) \propto \exp\left(\frac{1}{\alpha} Q^(s,a)\right)
$$

So α acts as a temperature:
	•	High α → flatter distribution → more exploration
	•	Low α → sharper distribution → more exploitation

---

# Full SAC Algorithm

Here’s a simplified pseudo-code version of SAC:

```
Algorithm 1: Soft Actor-Critic (SAC)
Initialize policy parameters θ, Q-function parameters φ₁, φ₂, temperature α
repeat
    Observe state s
    Sample action a ~ π_θ(a|s)
    Execute action a, observe reward r and next state s'
    Store (s, a, r, s') in replay buffer D
    Sample batch B = {(s, a, r, s')} from D
    Update critics φ₁, φ₂ using Bellman backup:
        y = r + γ * min_i Q_{φ_i}(s', a') - α * log π_θ(a'|s')
    Update actor θ using:
        ∇_θ J(θ) = ∇_θ [α * log π_θ(a|s) - Q_φ(s,a)]
    Adjust temperature α:
        ∇_α J(α) = -α * (log π_θ(a|s) + H_target)
until convergence
```

---

# Implementation Insights

The key insight in SAC is the interplay between three components:
	•	The soft Q-function estimates value under entropy regularization.
	•	The policy network seeks to minimize the expected α-weighted KL divergence.
	•	The temperature α adapts automatically to maintain a fixed entropy target.

Together, these make SAC:
	•	Stable: fewer learning crashes compared to DDPG/TD3.
	•	Efficient: reuses experience through an off-policy replay buffer.
	•	Automatic: minimal hyperparameter tuning for entropy.

--- 

# Practical Considerations
	•	Target Entropy:
Often set to ( \mathcal{H}_{\text{target}} = -\text{dim}(A) ), where A is the action space dimension.
For a 1D action space, that’s −1.0; for 6D, −6.0.
	•	Twin Q-Networks:
SAC uses two critics (like TD3) to mitigate overestimation bias.
	•	Reparameterization Trick:
Actions are sampled as ( a = \mu(s) + \sigma(s)\epsilon, ; \epsilon \sim \mathcal{N}(0,1) ),
allowing gradients to flow through stochastic actions.

---

# Visualization Example: Entropy Change Over Training

To get a sense of how α evolves, here’s a toy example simulating α adapting as entropy changes:

```{python}
import numpy as np
import matplotlib.pyplot as plt

steps = np.arange(200)
target_entropy = -1.0
log_alpha = 0.0
alpha = np.exp(log_alpha)

entropies = np.linspace(0.0, -2.0, len(steps))
alphas = []

for entropy in entropies:
    alpha_loss = -(log_alpha * (entropy - target_entropy))
    log_alpha -= 0.01 * alpha_loss  # gradient step
    alpha = np.exp(log_alpha)
    alphas.append(alpha)

plt.plot(steps, alphas)
plt.xlabel("Training Steps")
plt.ylabel("Alpha Value")
plt.title("Adaptive Temperature α Over Time")
plt.grid(True)
plt.show()
```

You’ll see α increase when entropy drops below the target and decrease when it’s too high — a perfect self-correcting mechanism.

---

# Why It Works So Well

SAC’s success stems from combining three major strengths:
	1.	Entropy regularization improves exploration.
	2.	Off-policy updates improve sample efficiency.
	3.	Automatic temperature tuning stabilizes learning.

This triad makes SAC remarkably robust and effective in diverse continuous control benchmarks like HalfCheetah, Hopper, and Walker2d.

---

# Conclusion

Soft Actor-Critic represents a fundamental shift in reinforcement learning design.
By optimizing for expected reward under a constraint on entropy, SAC provides a principled, practical way to achieve stable and efficient learning.

Key takeaways:
	•	SAC maximizes both reward and uncertainty.
	•	The temperature α is learned, not tuned.
	•	Entropy constraints make learning more adaptive and stable.

If you’ve struggled with brittle RL agents that either overfit or fail to explore, SAC’s entropy-constrained formulation offers a robust, elegant solution.

---

# References
	•	Haarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018).
Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.
[Paper]
	•	Haarnoja, T., et al. (2019).
Soft Actor-Critic Algorithms and Applications.
[Paper]