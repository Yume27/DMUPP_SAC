{"title":"Understanding Soft Actor-Critic: Entropy-Constrained Reinforcement Learning","markdown":{"yaml":{"title":"Understanding Soft Actor-Critic: Entropy-Constrained Reinforcement Learning","author":"Yumeka Nagano","format":{"html":{"toc":true,"toc-depth":3,"code-fold":true,"theme":"flatly"}},"jupyter":"python3","execute":{"echo":true,"eval":true}},"headingText":"Introduction","containsRefs":false,"markdown":"\n\nIn this post, I explore the key ideas behind the Soft Actor-Critic algorithm — how it uses entropy regularization to achieve stable and efficient learning, and why its automatic temperature adjustment is a breakthrough in continuous control RL.\n\n![](images/intro2.png){width=100%}\n\n\nThe **Soft Actor-Critic (SAC)** algorithm has become one of the most reliable and widely used methods in deep reinforcement learning (RL), especially for continuous control tasks.  \nAt first glance, SAC may look similar to other actor-critic algorithms like DDPG or TD3—but its key innovation lies in how it **optimizes not just reward, but also entropy**.\n\nIn this post, we’ll explore the main ideas behind SAC as introduced in *Soft Actor-Critic Algorithms and Applications* (Haarnoja et al., 2018).  \nWe’ll focus on two core features that make SAC both stable and efficient:\n\n1. **Entropy-constrained learning:** SAC doesn’t just balance reward and entropy with a fixed coefficient—it enforces a target entropy constraint.  \n2. **Automatic temperature adjustment:** The temperature parameter α is learned, not manually tuned.\n\nBy the end, we’ll see why this approach makes SAC more robust, sample-efficient, and less sensitive to hyperparameters than many prior methods.\n\n---\n\n# Motivation: Beyond Deterministic Control\n\nTraditional actor-critic algorithms (like DDPG) are **deterministic**: the policy outputs a specific action for each state.  \nThis is efficient for exploitation, but risky for exploration—especially in continuous action spaces.\n\nEntropy-regularized reinforcement learning introduces a **stochastic policy** that maximizes not only expected reward but also **policy entropy**:\n\n$$\nJ(\\pi) = \\mathbb{E}_{(s,a)\\sim\\rho_\\pi} [r(s,a) + \\alpha \\mathcal{H}(\\pi(\\cdot|s))]\n$$\n\nHere:\n\n- \\( \\mathcal{H}(\\pi(\\cdot|s)) = - \\mathbb{E}_{a\\sim\\pi} [\\log \\pi(a|s)] \\) measures how random the policy is.\n- \\( \\alpha \\) controls the trade-off between reward and entropy.\n\nHigher entropy encourages exploration and smoother learning, but if \\( \\alpha \\) is too high, the agent behaves almost randomly.  \nSo, how do we pick α?\n\n---\n\n# Key Structure\n![](images/structure.png){width=100%}\n\n---\n\n# Fixed vs. Adaptive Temperature\n\nEarly versions of entropy-regularized RL used a **fixed α**, chosen via hyperparameter tuning.  \nThis is suboptimal for two reasons:\n\n1. The “right” level of entropy depends on the environment and the stage of learning.  \n2. A fixed α couples exploration strength directly to reward scaling.\n\nSAC resolves this beautifully by treating α as a **learnable parameter**.\n\n---\n\n# Automatic Temperature Adjustment\n\nThe SAC paper reframes α as a variable that enforces a **target entropy constraint**:\n\n$$\n\\mathcal{L}(\\alpha) = \\mathbb{E}_{a_t \\sim \\pi_t} [-\\alpha (\\log \\pi_t(a_t|s_t) + \\mathcal{H}_{\\text{target}})]\n$$\n\nThis objective pushes α to increase if the policy entropy is below the target and decrease otherwise.\n\nIn PyTorch, the update step looks like this:\n\n```{python}\nimport torch\n\n# Initialize log_alpha (so α = exp(log_alpha))\nlog_alpha = torch.tensor(0.0, requires_grad=True)\nalpha_optimizer = torch.optim.Adam([log_alpha], lr=3e-4)\n\n# Example of computing the temperature loss\nlog_prob = torch.tensor([-1.2, -0.8, -1.5])  # log π(a|s) for some batch\ntarget_entropy = -1.0  # typical target for 1D continuous action\n\nalpha_loss = -(log_alpha * (log_prob + target_entropy).detach()).mean()\nalpha_optimizer.zero_grad()\nalpha_loss.backward()\nalpha_optimizer.step()\n\nalpha = log_alpha.exp().item()\nalpha\n```\n\nThe optimizer adjusts log_alpha so that the expected policy entropy matches the desired target.\n\n---\n\n# Why Entropy Constraint Matters\n\nInstead of weighting reward and entropy with a fixed scalar α, SAC enforces:\n\n$$\n\\mathcal{H}(\\pi(\\cdot|s_t)) \\approx \\mathcal{H}_{\\text{target}}\n$$\n\nThis dynamic adjustment stabilizes learning in several ways:\n\n- Prevents premature convergence (too little exploration).\n- Avoids oscillation or instability from excessive randomness.\n- Keeps policy behavior consistent across environments and reward scales.\n\nIntuitively, it’s like giving the agent an adaptive curiosity level that balances exploration with exploitation automatically.\n\n---\n\n# Entropy–Temperature Relationship\n\nThe relationship between entropy and α can be visualized simply:\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nentropies = np.linspace(0.5, 2.0, 100)\nalphas = 1 / entropies\n\nplt.plot(entropies, alphas)\nplt.xlabel(\"Entropy\")\nplt.ylabel(\"Temperature α\")\nplt.title(\"Entropy–Temperature Relationship in SAC\")\nplt.grid(True)\nplt.show()\n```\n\nWhen the policy’s entropy decreases (less randomness), α increases—pushing the policy to become more stochastic again.\nThis feedback loop maintains a balanced level of exploration throughout training.\n\n---\n\n# The Soft Q-Function\n\nSAC also modifies the Bellman backup to incorporate entropy:\n\n$$\nQ^{\\pi}(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}{s{t+1}\\sim p, a_{t+1}\\sim\\pi} [Q^{\\pi}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi(a_{t+1}|s_{t+1})]\n$$\n\nThis soft Q-function estimates the expected return plus an entropy bonus.\nThe inclusion of ( -\\alpha \\log \\pi(a|s) ) encourages higher entropy where possible, while still optimizing reward.\n\n---\n\n# Policy Update\n\nThe policy update is also softened:\n\n$$\n\\pi_{\\text{new}} = \\arg\\min_\\pi D_{\\text{KL}}\\left(\n\\pi(\\cdot|s_t) ;||;\n\\frac{\\exp(\\frac{1}{\\alpha} Q(s_t, \\cdot))}{Z(s_t)}\n\\right)\n$$\n\nThis means the optimal policy should be close to a Boltzmann distribution over actions:\n\n$$\n\\pi^(a|s) \\propto \\exp\\left(\\frac{1}{\\alpha} Q^(s,a)\\right)\n$$\n\nSo α acts as a temperature:\n\t•\tHigh α → flatter distribution → more exploration\n\t•\tLow α → sharper distribution → more exploitation\n\n---\n\n# Full SAC Algorithm\n\nHere’s a simplified pseudo-code version of SAC:\n\n```\nAlgorithm 1: Soft Actor-Critic (SAC)\nInitialize policy parameters θ, Q-function parameters φ₁, φ₂, temperature α\nrepeat\n    Observe state s\n    Sample action a ~ π_θ(a|s)\n    Execute action a, observe reward r and next state s'\n    Store (s, a, r, s') in replay buffer D\n    Sample batch B = {(s, a, r, s')} from D\n    Update critics φ₁, φ₂ using Bellman backup:\n        y = r + γ * min_i Q_{φ_i}(s', a') - α * log π_θ(a'|s')\n    Update actor θ using:\n        ∇_θ J(θ) = ∇_θ [α * log π_θ(a|s) - Q_φ(s,a)]\n    Adjust temperature α:\n        ∇_α J(α) = -α * (log π_θ(a|s) + H_target)\nuntil convergence\n```\n\n---\n\n# Implementation Insights\n\nThe key insight in SAC is the interplay between three components:\n\t•\tThe soft Q-function estimates value under entropy regularization.\n\t•\tThe policy network seeks to minimize the expected α-weighted KL divergence.\n\t•\tThe temperature α adapts automatically to maintain a fixed entropy target.\n\nTogether, these make SAC:\n\t•\tStable: fewer learning crashes compared to DDPG/TD3.\n\t•\tEfficient: reuses experience through an off-policy replay buffer.\n\t•\tAutomatic: minimal hyperparameter tuning for entropy.\n\n--- \n\n# Practical Considerations\n\t•\tTarget Entropy:\nOften set to ( \\mathcal{H}_{\\text{target}} = -\\text{dim}(A) ), where A is the action space dimension.\nFor a 1D action space, that’s −1.0; for 6D, −6.0.\n\t•\tTwin Q-Networks:\nSAC uses two critics (like TD3) to mitigate overestimation bias.\n\t•\tReparameterization Trick:\nActions are sampled as ( a = \\mu(s) + \\sigma(s)\\epsilon, ; \\epsilon \\sim \\mathcal{N}(0,1) ),\nallowing gradients to flow through stochastic actions.\n\n---\n\n# Visualization Example: Entropy Change Over Training\n\nTo get a sense of how α evolves, here’s a toy example simulating α adapting as entropy changes:\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsteps = np.arange(200)\ntarget_entropy = -1.0\nlog_alpha = 0.0\nalpha = np.exp(log_alpha)\n\nentropies = np.linspace(0.0, -2.0, len(steps))\nalphas = []\n\nfor entropy in entropies:\n    alpha_loss = -(log_alpha * (entropy - target_entropy))\n    log_alpha -= 0.01 * alpha_loss  # gradient step\n    alpha = np.exp(log_alpha)\n    alphas.append(alpha)\n\nplt.plot(steps, alphas)\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Alpha Value\")\nplt.title(\"Adaptive Temperature α Over Time\")\nplt.grid(True)\nplt.show()\n```\n\nYou’ll see α increase when entropy drops below the target and decrease when it’s too high — a perfect self-correcting mechanism.\n\n---\n\n# Why It Works So Well\n\nSAC’s success stems from combining three major strengths:\n\t1.\tEntropy regularization improves exploration.\n\t2.\tOff-policy updates improve sample efficiency.\n\t3.\tAutomatic temperature tuning stabilizes learning.\n\nThis triad makes SAC remarkably robust and effective in diverse continuous control benchmarks like HalfCheetah, Hopper, and Walker2d.\n\n---\n\n# Conclusion\n\nSoft Actor-Critic represents a fundamental shift in reinforcement learning design.\nBy optimizing for expected reward under a constraint on entropy, SAC provides a principled, practical way to achieve stable and efficient learning.\n\nKey takeaways:\n\t•\tSAC maximizes both reward and uncertainty.\n\t•\tThe temperature α is learned, not tuned.\n\t•\tEntropy constraints make learning more adaptive and stable.\n\nIf you’ve struggled with brittle RL agents that either overfit or fail to explore, SAC’s entropy-constrained formulation offers a robust, elegant solution.\n\n---\n\n# References\n\t•\tHaarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018).\nSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\n[Paper]\n\t•\tHaarnoja, T., et al. (2019).\nSoft Actor-Critic Algorithms and Applications.\n[Paper]","srcMarkdownNoYaml":"\n\nIn this post, I explore the key ideas behind the Soft Actor-Critic algorithm — how it uses entropy regularization to achieve stable and efficient learning, and why its automatic temperature adjustment is a breakthrough in continuous control RL.\n\n![](images/intro2.png){width=100%}\n\n# Introduction\n\nThe **Soft Actor-Critic (SAC)** algorithm has become one of the most reliable and widely used methods in deep reinforcement learning (RL), especially for continuous control tasks.  \nAt first glance, SAC may look similar to other actor-critic algorithms like DDPG or TD3—but its key innovation lies in how it **optimizes not just reward, but also entropy**.\n\nIn this post, we’ll explore the main ideas behind SAC as introduced in *Soft Actor-Critic Algorithms and Applications* (Haarnoja et al., 2018).  \nWe’ll focus on two core features that make SAC both stable and efficient:\n\n1. **Entropy-constrained learning:** SAC doesn’t just balance reward and entropy with a fixed coefficient—it enforces a target entropy constraint.  \n2. **Automatic temperature adjustment:** The temperature parameter α is learned, not manually tuned.\n\nBy the end, we’ll see why this approach makes SAC more robust, sample-efficient, and less sensitive to hyperparameters than many prior methods.\n\n---\n\n# Motivation: Beyond Deterministic Control\n\nTraditional actor-critic algorithms (like DDPG) are **deterministic**: the policy outputs a specific action for each state.  \nThis is efficient for exploitation, but risky for exploration—especially in continuous action spaces.\n\nEntropy-regularized reinforcement learning introduces a **stochastic policy** that maximizes not only expected reward but also **policy entropy**:\n\n$$\nJ(\\pi) = \\mathbb{E}_{(s,a)\\sim\\rho_\\pi} [r(s,a) + \\alpha \\mathcal{H}(\\pi(\\cdot|s))]\n$$\n\nHere:\n\n- \\( \\mathcal{H}(\\pi(\\cdot|s)) = - \\mathbb{E}_{a\\sim\\pi} [\\log \\pi(a|s)] \\) measures how random the policy is.\n- \\( \\alpha \\) controls the trade-off between reward and entropy.\n\nHigher entropy encourages exploration and smoother learning, but if \\( \\alpha \\) is too high, the agent behaves almost randomly.  \nSo, how do we pick α?\n\n---\n\n# Key Structure\n![](images/structure.png){width=100%}\n\n---\n\n# Fixed vs. Adaptive Temperature\n\nEarly versions of entropy-regularized RL used a **fixed α**, chosen via hyperparameter tuning.  \nThis is suboptimal for two reasons:\n\n1. The “right” level of entropy depends on the environment and the stage of learning.  \n2. A fixed α couples exploration strength directly to reward scaling.\n\nSAC resolves this beautifully by treating α as a **learnable parameter**.\n\n---\n\n# Automatic Temperature Adjustment\n\nThe SAC paper reframes α as a variable that enforces a **target entropy constraint**:\n\n$$\n\\mathcal{L}(\\alpha) = \\mathbb{E}_{a_t \\sim \\pi_t} [-\\alpha (\\log \\pi_t(a_t|s_t) + \\mathcal{H}_{\\text{target}})]\n$$\n\nThis objective pushes α to increase if the policy entropy is below the target and decrease otherwise.\n\nIn PyTorch, the update step looks like this:\n\n```{python}\nimport torch\n\n# Initialize log_alpha (so α = exp(log_alpha))\nlog_alpha = torch.tensor(0.0, requires_grad=True)\nalpha_optimizer = torch.optim.Adam([log_alpha], lr=3e-4)\n\n# Example of computing the temperature loss\nlog_prob = torch.tensor([-1.2, -0.8, -1.5])  # log π(a|s) for some batch\ntarget_entropy = -1.0  # typical target for 1D continuous action\n\nalpha_loss = -(log_alpha * (log_prob + target_entropy).detach()).mean()\nalpha_optimizer.zero_grad()\nalpha_loss.backward()\nalpha_optimizer.step()\n\nalpha = log_alpha.exp().item()\nalpha\n```\n\nThe optimizer adjusts log_alpha so that the expected policy entropy matches the desired target.\n\n---\n\n# Why Entropy Constraint Matters\n\nInstead of weighting reward and entropy with a fixed scalar α, SAC enforces:\n\n$$\n\\mathcal{H}(\\pi(\\cdot|s_t)) \\approx \\mathcal{H}_{\\text{target}}\n$$\n\nThis dynamic adjustment stabilizes learning in several ways:\n\n- Prevents premature convergence (too little exploration).\n- Avoids oscillation or instability from excessive randomness.\n- Keeps policy behavior consistent across environments and reward scales.\n\nIntuitively, it’s like giving the agent an adaptive curiosity level that balances exploration with exploitation automatically.\n\n---\n\n# Entropy–Temperature Relationship\n\nThe relationship between entropy and α can be visualized simply:\n\n```{python}\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nentropies = np.linspace(0.5, 2.0, 100)\nalphas = 1 / entropies\n\nplt.plot(entropies, alphas)\nplt.xlabel(\"Entropy\")\nplt.ylabel(\"Temperature α\")\nplt.title(\"Entropy–Temperature Relationship in SAC\")\nplt.grid(True)\nplt.show()\n```\n\nWhen the policy’s entropy decreases (less randomness), α increases—pushing the policy to become more stochastic again.\nThis feedback loop maintains a balanced level of exploration throughout training.\n\n---\n\n# The Soft Q-Function\n\nSAC also modifies the Bellman backup to incorporate entropy:\n\n$$\nQ^{\\pi}(s_t, a_t) = r(s_t, a_t) + \\gamma \\mathbb{E}{s{t+1}\\sim p, a_{t+1}\\sim\\pi} [Q^{\\pi}(s_{t+1}, a_{t+1}) - \\alpha \\log \\pi(a_{t+1}|s_{t+1})]\n$$\n\nThis soft Q-function estimates the expected return plus an entropy bonus.\nThe inclusion of ( -\\alpha \\log \\pi(a|s) ) encourages higher entropy where possible, while still optimizing reward.\n\n---\n\n# Policy Update\n\nThe policy update is also softened:\n\n$$\n\\pi_{\\text{new}} = \\arg\\min_\\pi D_{\\text{KL}}\\left(\n\\pi(\\cdot|s_t) ;||;\n\\frac{\\exp(\\frac{1}{\\alpha} Q(s_t, \\cdot))}{Z(s_t)}\n\\right)\n$$\n\nThis means the optimal policy should be close to a Boltzmann distribution over actions:\n\n$$\n\\pi^(a|s) \\propto \\exp\\left(\\frac{1}{\\alpha} Q^(s,a)\\right)\n$$\n\nSo α acts as a temperature:\n\t•\tHigh α → flatter distribution → more exploration\n\t•\tLow α → sharper distribution → more exploitation\n\n---\n\n# Full SAC Algorithm\n\nHere’s a simplified pseudo-code version of SAC:\n\n```\nAlgorithm 1: Soft Actor-Critic (SAC)\nInitialize policy parameters θ, Q-function parameters φ₁, φ₂, temperature α\nrepeat\n    Observe state s\n    Sample action a ~ π_θ(a|s)\n    Execute action a, observe reward r and next state s'\n    Store (s, a, r, s') in replay buffer D\n    Sample batch B = {(s, a, r, s')} from D\n    Update critics φ₁, φ₂ using Bellman backup:\n        y = r + γ * min_i Q_{φ_i}(s', a') - α * log π_θ(a'|s')\n    Update actor θ using:\n        ∇_θ J(θ) = ∇_θ [α * log π_θ(a|s) - Q_φ(s,a)]\n    Adjust temperature α:\n        ∇_α J(α) = -α * (log π_θ(a|s) + H_target)\nuntil convergence\n```\n\n---\n\n# Implementation Insights\n\nThe key insight in SAC is the interplay between three components:\n\t•\tThe soft Q-function estimates value under entropy regularization.\n\t•\tThe policy network seeks to minimize the expected α-weighted KL divergence.\n\t•\tThe temperature α adapts automatically to maintain a fixed entropy target.\n\nTogether, these make SAC:\n\t•\tStable: fewer learning crashes compared to DDPG/TD3.\n\t•\tEfficient: reuses experience through an off-policy replay buffer.\n\t•\tAutomatic: minimal hyperparameter tuning for entropy.\n\n--- \n\n# Practical Considerations\n\t•\tTarget Entropy:\nOften set to ( \\mathcal{H}_{\\text{target}} = -\\text{dim}(A) ), where A is the action space dimension.\nFor a 1D action space, that’s −1.0; for 6D, −6.0.\n\t•\tTwin Q-Networks:\nSAC uses two critics (like TD3) to mitigate overestimation bias.\n\t•\tReparameterization Trick:\nActions are sampled as ( a = \\mu(s) + \\sigma(s)\\epsilon, ; \\epsilon \\sim \\mathcal{N}(0,1) ),\nallowing gradients to flow through stochastic actions.\n\n---\n\n# Visualization Example: Entropy Change Over Training\n\nTo get a sense of how α evolves, here’s a toy example simulating α adapting as entropy changes:\n\n```{python}\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nsteps = np.arange(200)\ntarget_entropy = -1.0\nlog_alpha = 0.0\nalpha = np.exp(log_alpha)\n\nentropies = np.linspace(0.0, -2.0, len(steps))\nalphas = []\n\nfor entropy in entropies:\n    alpha_loss = -(log_alpha * (entropy - target_entropy))\n    log_alpha -= 0.01 * alpha_loss  # gradient step\n    alpha = np.exp(log_alpha)\n    alphas.append(alpha)\n\nplt.plot(steps, alphas)\nplt.xlabel(\"Training Steps\")\nplt.ylabel(\"Alpha Value\")\nplt.title(\"Adaptive Temperature α Over Time\")\nplt.grid(True)\nplt.show()\n```\n\nYou’ll see α increase when entropy drops below the target and decrease when it’s too high — a perfect self-correcting mechanism.\n\n---\n\n# Why It Works So Well\n\nSAC’s success stems from combining three major strengths:\n\t1.\tEntropy regularization improves exploration.\n\t2.\tOff-policy updates improve sample efficiency.\n\t3.\tAutomatic temperature tuning stabilizes learning.\n\nThis triad makes SAC remarkably robust and effective in diverse continuous control benchmarks like HalfCheetah, Hopper, and Walker2d.\n\n---\n\n# Conclusion\n\nSoft Actor-Critic represents a fundamental shift in reinforcement learning design.\nBy optimizing for expected reward under a constraint on entropy, SAC provides a principled, practical way to achieve stable and efficient learning.\n\nKey takeaways:\n\t•\tSAC maximizes both reward and uncertainty.\n\t•\tThe temperature α is learned, not tuned.\n\t•\tEntropy constraints make learning more adaptive and stable.\n\nIf you’ve struggled with brittle RL agents that either overfit or fail to explore, SAC’s entropy-constrained formulation offers a robust, elegant solution.\n\n---\n\n# References\n\t•\tHaarnoja, T., Zhou, A., Abbeel, P., & Levine, S. (2018).\nSoft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor.\n[Paper]\n\t•\tHaarnoja, T., et al. (2019).\nSoft Actor-Critic Algorithms and Applications.\n[Paper]"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"engine":"jupyter"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":true,"code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"output-file":"sac.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.8.25","theme":"flatly","title":"Understanding Soft Actor-Critic: Entropy-Constrained Reinforcement Learning","author":"Yumeka Nagano","jupyter":"python3"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}